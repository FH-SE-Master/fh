\chapter{Clustering}\label{Clustering}
\section{Einführung}
In der Informatik und Statistik sind große Datensätze anzutreffen. Diese können analysiert werden, wenn entsprechende Tools und Algorithmen zur Verfügung stehen. Durch das Verfahren von Clustering und Bildung von eingeteilten Datensätzen wird das Arbeiten mit großen Daten erleichtert. Das Ziel ist ein geeignetes Modell für einen gegeben Datensatz zu finden. Durch diese Methoden wird das Interpretieren von Merkmalen und Besonderheiten erleichtert und ermöglicht außerdem eine begünstigte Aufbereitung von den Daten.\cite{fasulo99}\\Um die richtigen Modellrepräsentationen  zu finden und diese zu identifizieren ist es notwendig die Daten zuerst in Klassen einzuteilen. Damit beschäftigt sich die \nameref{Klassifikation}. Dies ist sehr hilfreich wenn ein klassenorientiertes Clustering vorgenommen wird, da die Einteilung zu relevanten Merkmalen zuerst erfolgen muss.
Dabei wird zwischen zwei Arten unterschieden: Die überwachte (\textit{supervised}) und die unüberwachte(\textit{unsupervised}) Klassifikation.\cite{fasulo99} \\
Clustering gehört zu der unüberwachten Klassifikation, welche die Daten in relevante Klassen sogenannte Cluster einteilt. \\
Das Einsatzgebiet von Clustering-Algorithmen ist vielfältig. Die Anwendungsgebiete sind vor allem im Gebiet von Data-Mining und Data-Analysis zu finden. Weitere Anwendungsgebiete sind die Bioinformatik, Analyse von Datenbanken, Textmining u. Neuronale Netzwerke. Clustering spielt bei der Datenanalyse eine große und bedeutende Rolle.\cite{fasulo99} 



\section{Datenrepräsentation}
Als \textsl{Daten} wird eine Ansammlung bzw. eine Menge an Dingen oder Objekten bezeichnet. Diese Definition bezieht sich auf den Zusammenhang von Daten und Clustering. Jedes einzelne Objekt besitzt spezielle Eigenschaften, welche eindeutig per Objekttyp sein können. Diese Eigenschaften werden oft auch als Attribute, Merkmale oder Dimensionen bezeichnet.\cite{pell91}\\In einen Datenraum befinden sich Objekte oder Elemente mit einer endlichen Anzahl von Merkmalen, welche bei allen Objekten denen des Datenraumes gleichen. Jedoch können sich Daten die sich innerhalb dieses vorgegebenen Raumes befinden sich unterscheiden, da die Ausprägung nicht vorgegeben ist. \cite{pell91}
Dabei gibt es eine mathematische Repräsentation:\\


\noindent\(D,D_d:\) Datenraum (auch Merkmalsraum) der Dimension \textit{d} (auch \(\mathbb{R}^d\))\\

\noindent\(S:\) Datenmenge, \(S \subset D \)\\

\noindent\(x_i:\) \textsl{i}-tes Objekt aus \(S\)\\

\noindent\(|S|,n:\) Mächtigkeit von S (Anzahl von Objekten)\\[5px]

\def\tmp{%
\begin{matrix}
  x_1= &(  x_{1,1} & \cdots & a_{1,d} ) \\
 x_2 = & (  x_{2,1} & \cdots & a_{2,d} ) \\
\vdots &   \vdots  & \ddots & \vdots  \\
 x_n & ( x_{n,1} & \cdots & a_{n,d}  )


\end{matrix}
}%
\[  
\stackMath\def\stackalignment{r}%
  \stackon%
    {\mathrm{\text{Objekte}}\left\{\tmp\right.}%
    {\overbrace{\phantom{\smash{\tmp\mkern -82mu}}}^{\mathrm{\textstyle Attribute}}\mkern 15mu}
\]\\[5px]

Jedes einzelne Objekt aus der Datenmenge \textsl{S} hat \textsl{d} Merkmale/Attribute. Diese werden durch die Art (des Typs) unterschieden. Dabei wird zwischen \textit{numerisch} und \textit{nicht-numerisch} Daten unterschieden. \\

Erstens betroffen sind die Daten bzw. Objekte in Vektoren von reelwertigen Zahlen. Als Beispiel sei eine Zeitreihe von einer Messung genannt.  Zweitens betrifft den Rest, welcher in ein numerisches Format übertragen werden kann. Im Allgemeinen können \textit{nicht-numerische} Merkmale durch spezifische Codierungen in numerische Merkmale übertragen werden. Dabei wird jedes einzelne Merkmal durch ein oder mehrere Attribute repräsentativ dargestellt.\cite{pell91}\\ 
Diese Methode wird verwendet um \textit{numerische} und \textit{nicht-numerische} Daten gleich zu behandeln, da es dabei keinen Unterschied in der Anwendung bzw. Auswertung gibt. Diese Methode ist notwendig um die Daten in ein Format zu bringen, welches für das Clustering verwendet werden kann. Dann können die Daten von einem geeigneten Algorithmus verarbeitet werden.\cite{pell91}\\ 
Doch in der realen Welt sind große und komplexe Daten an der Tagesordnung. Doch im Gegensatz dazu würde es sich sehr aufwändig gestalten, wenn  Algorithmen mit hoher Dimensionalität an Daten verwendet werden. Da sich dabei die Rechenzeit erheblich erhöhen würde müssen Methoden angewendet werden um die Datenkomplexität zu reduzieren. Bei vielen Datensätzen sind  zu viele Merkmale vorhanden, die sogar irrelevant für die Berechnung sind. Manche können den Algorithmus sogar in eine falsche Richtung führen.\cite{pell91}\\
Einerseits gibt es die Möglichkeit gewisse Merkmale auszuklammern d.h. diese werden nicht in die Berechnung aufgenommen um die Komplexität zu verringern. Dabei gehen keine wichtigen Attribute verloren. Dieses Verfahren wird als Merkmalsauswahl(\textit{featuren selection}) genannt. Es ist oft hilfreich nur gewisse Merkmale auszuwählen, um eine Selektion von den Besten zu ermöglichen (auch Elitismus genannt), denn dann sind die Algorithmen performant und liefern annehmbare Ergebnisse.\cite{pell91}\\ 
Eine weitere Möglichkeit ist es gewisse Merkmale aus einer Ansammlung auszuwählen. Dadurch kann auch eine Featurereduktion erreicht werden. Dies ist ebenso so effizient wie die Auswahl der Merkmale.  Diese Methode wird Merkmalsextraktion(\textit{feature extraction}) genannt. Dabei werden nur die wichtigsten Merkmale herangezogen, um die Performance zu steigern.\cite{pell91}\\
Neben den beiden Methoden ist es auch notwendig die Daten zu normalisieren d.h es müssen die Daten auf die gleiche Weise umgerechnet werden, damit sie besser zusammenpassen. Dies wird durch skalieren und konvertieren erreicht.  Die Normalverteilung der Daten wird angenommen um die Skalierung zu erleichtern; mit denselben Mittelwert (\(\bar{x}\)) und der selben Standardabweichung (\(\sigma\)), mit \(\bar{x} = 0\) und \(\sigma = 1\).\cite{pell91}\\
Bevor die Algorithmen angewendet werden können, muss ein geeignetes Maß für den Abstand gefunden werden.  Die Maße sind metrisch u. es wird daher der Überbegriff der Ähnlichkeitsbestimmung verwendet. Diese Maße geben an wie ähnlich sich zwei Objekte sind und  dabei wird nicht zwischen \textit{numerisch} und \textit{nicht-numerisch} unterschieden. Das Distanzmaß (\textit{distance measure}) oder Ähnlichkeitsmaß(\textit{similarity/proximity/affinity measure}) definiert die Beziehung bzw. die Funktion \(d: D \times D \rightarrow Z \). Die Maße sind essentiell für das Clustering und müssen vor den eigentlichen Clustering ausgeführt werden.\cite{fasulo99}

\section{Algorithmen}

\subsection{Allgemein}
Nachdem ein geeignetes Distanzmaß gefunden ist, kann ein bestimmter Algorithmus auf die Daten angewendet werden. Es gibt zwei Gruppen von Verfahren bzw. Algorithmen, welche das  \textit{Hierarchisches Clustering} und die \textit{Partitionierung} darstellen. Bei der Partitionierung werden die Objekte bzw. Daten in Gruppen eingeteilt und diese Gruppen enthalten keine weiteren verschachtelten Cluster und besitzen nur eine Ebene. Beim \textit{Hierarchischen Clustering} entsteht ein geschachtelter Aufbau bzw. eine Struktur, wo größere Cluster kleinere enthalten.\cite{fasulo99}\\

Weiters können die beiden oben angeführten Methoden weiter aufgegliedert werden:\cite{fasulo99}

\begin{itemize}
	\item\textit{divisiv:}\\
	Bei dieser Methode werden alle Objekte einen Cluster zugeordnet sowie schrittweise verkleinert, indem schrittweise zerteilt wird, bis ein vordefiniertes Abbruchkriterium eintritt.\\
	
	\item\textit{agglomerativ:}\\
	Im Gegensatz zu der divisiven Methode wird bei der agglomerativen Methode mit kleinen Clustern begonnen. Jedes Objekt stellt einen Cluster für sich dar. Die kleinen Cluster werden schrittweise zusammengefügt bis ein Abbruchkriterium eintritt.\\
	
	\item\textit{hard:}\\
	Algorithmen welche das Prinzip von einer strikten Vorgehensweise (\textit{hard}) verfolgen ordnen einen Cluster ein Objekt zu.\\
	
	\item\textit{fuzzy:}\\
	Im Gegenteil dazu gibt es das Prinzip von der ungenauen Vorgehensweise(\textit{fuzzy}), dabei können Objekte verschiedenen Clustern zugeordnet werden. \\
	
	\item\textit{stochastisch:}\\
		Bei dem Begriff stochastisch kann davon ausgegangen werden, dass der Zufall eine Rolle spielt und die Auswahl verschiedener Objekte oder Attribute keiner Regel folgt.\\
	
	
	\item\textit{deterministisch:}\\
	Dabei handelt es sich um die Vorgabe keiner zufälligen Ereignisse. Hier muss alles vorgegeben sein damit es als deterministisch gilt.\\
	
	\item\textit{monothetisch:}\\
	Wenn bei der Verarbeitung nur ein Cluster bzw. ein Objekt verarbeitet wird, dann wird dieses Verfahren monothetisch bezeichnet. Aber die Algorithmen arbeiten nur bedingt nach diesem Prinzip, da dadurch die Berechnungszeit erhöht sein kann.\\
	
		\item\textit{polythetisch:}\\
		Bei der polythetischen Vorgangsweise werden Cluster bzw. Objekte oder Daten schneller verarbeitet, da Vorgänge gleichzeitig ausgeführt werden. In Bezug auf das Clustering bezieht sich die Gleichzeitigkeit auf die Distanzberechnung der Merkmale. 
	
\end{itemize}

\subsection{k-means Algorithmus}
\subsubsection{Beschreibung:}
Der \textit{k-means} Algorithmus gehört zu den Partitionierungs-Algorithmen. Die Implementierung ist einfach und liefert trotzdem gut interpretierbare Ergebnisse für einfache Aufgabenstellungen. Grundlegend versucht der Algorithmus eine Partition in den Daten zu finden und daraus dann Cluster zu bilden. Die Anzahl der Cluster wird durch den Anwender festgelegt und während der Laufzeit nicht mehr geändert. Die Formel nach dem die Cluster gebildet werden lautet: \cite{pell91} \\

\[x_r^i: r\textrm{-te Element des Clusters}\ C_i\]
Diese Methode wird auch als Sum-of-Squares bezeichnet. Dabei werden die quadratischen Abstände minimiert. Beim Clustering bedeutet dies, dass die Ähnlichkeit der Attribute, Merkmale oder Objekte bestimmt wird. Damit basieren Cluster auf der oben angeführten Kostenfunktion.

\subsubsection{Algorithmus:}
\begin{enumerate}
\item Wähle zufällig \(k\) Cluster-Zentren \(\mu_1,\dotsc,\mu_k\).\\


\item Berechne für jedes \(x \in S\), zu welchen Clustermittelpunkt \(\mu_i\) es am nächsten liegt.\\


\item Berechne für jeden Cluster \(C_i\) die Kostenfunktion:\\
\[c(C_i) = \displaystyle\sum_{r=1}^{|C_i|}(d(\mu_i,x_r^i))^2\]\\

\item Berechne für jeden Cluster \(C_i\) den eigenen neuen Mittelpunk:\\
\[ \mu_i =\frac{1}{|C_i|} \displaystyle\sum_{r=1}^{|C_i|}x_r^i\]\\

\item Wiederhole 2., 3., 4. bis sich die Clusterzuordnung nicht mehr ändert.\\




\end{enumerate}

Die Datensätze besitzen einen Mittelwert, da diese \textit{numerisch} sind. Daher kann ein Mittelwert oder auch das arithmetisches bzw. geometrisches Mittel gebildet werden. Auch beim Clustering können \textit{nicht-numerische} Datensätze verwendet werden. Diese besitzen meistens keinen numerischen Mittelwert.\\

Dennoch kann ein Lagemaß berechnet werden: der Median. Dieser gibt ähnlich wie der Mittelwert eine gute Aussage wie die Daten verteilt sind und es können auch damit \textit{nicht-numerischen} Daten berechnet werden. Beim Clustering wird der ähnliche \textit{k-medioids} Algorithmus angewandt, welcher  nach dem oben genannten Prinzip funktioniert. Nur wird bei der Berechnung der Mittelwert \(\mu\) durch den Median ersetzt. \cite{pell91}

\subsubsection{Zusammenfassung}
Dieser Clusteringalgorithmus ist einfach in seiner Komplexität, da er sich nur auf die Mittelwerte der einzelnen Attribute bezieht. Doch bei der Bildung von Clustern können einfach sphärische Cluster entstehen, da die Berechnung relativ zum Mittelwert geschieht. Weiteres kann ein vorhandenes Rauschen in den Daten (Störung in den Daten) bei diesem Algorithmus nicht beseitigt werden, da der Mittelwert sehr ausreißerempfindlich ist.


\subsection{Hierarchisch Clustering}
\subsubsection{Beschreibung:}
Diese Methode wird verwendet wenn die Daten nicht offensichtlich in Gruppen bzw. Partitionen eingeteilt sind oder es keine separierte Cluster gibt. Diese Methode erstellt eine hierarchische Baumstruktur der Datenmenge. Dabei sind die einzelnen Knoten bzw. Enden jeweils eine Teilmenge des übergeordneten Knotens. Der Wurzelknoten repräsentiert die gesamte Menge und die Blätter die einzelnen Objekte.\\

Bei diesem Algorithmus werden \textit{bottom-up} und \textit{top-down} als Verfahren unterschieden\\

Bei der \textit{bottom-up} Methode wird anfangs von kleinen Elementen bzw. Clustern ausgegangen. Diese werden immer weiter kombiniert bis ein gemeinsamer Megacluster entsteht, welcher den ganzen Datensatz enthält. Wenn ein großer Cluster entstanden ist, ist der Algorithmus durchlaufen bzw. das Clustering abgeschlossen.\cite{jain99, mitch97}\\

Im Gegensatz dazu wird bei der \textit{top-down} Methode von einen einzelnen Cluster ausgegangen, welcher den ganzen Datensatz enthält. Hier wird schrittweise der übergeordnete Cluster, auch \textit{parent} genannt in mehrere kleinere Cluster (\textit{child})zerlegt. Diese stellen den Eltern-Cluster dar. Wenn in jeden Cluster nur mehr ein Element vorhanden ist, ist das Clustering abgeschlossen.\\

Beide Methoden verwenden eine Baumstruktur im Hintergrund, in der die einzelnen Cluster als Knoten repräsentiert werden. Dadurch kann mit größeren und komplexeren Datensätzen gearbeitet werden. \cite{jain99, mitch97}

\subsubsection{Algorithmus}

Der Algorithmus wird anhand der der \textit{bottom-up} Methode erklärt. Dabei wird eine Vereinigung von zwei Clustern verwendet. Die zweite Methode \textit{top-down} kann durch durch teilen der Cluster beschrieben werden:\\

\begin{enumerate}
	\item Beginne mit \(n\) Clustern \(C_1,\dotsc,C_n\); wobei \(C_i = x_i\).\\
	
	\item Minimiere die Kostenfunktion \(c(C_i,C_j)\), um die \textsl{beste} bzw. \textsl{günstigste} Vereinigung ( \(C_i \cup C_j\)) zu finden.\\
	
	\item Ersetze \(C_i\) und \(C_j\) durch die Vereinigung \(C_i \cup C_j\).\\
	
	\item Wiederhole die Schritte 2 und 3 bis alle Cluster zusammengefasst sind. \\
	

\end{enumerate}


Das hierarchische Clustering ist nur ein Verfahren, dass einzelne Algorithmen implementiert. Die Vorgehensweise  unterscheidet sich nur in der Ausführung des Verfahrens. Es wird unterschieden zwischen \textit{Single-Link}, \textit{Average-Link} und \textit{Complete-Link}. Dabei unterscheiden sich die Verfahren nur in der Kostenfunktion:\\

\begin{itemize}
	\item \textit{Single-Link}:\\
	\[c(C_i,C_j) = \min\limits_{x \in C_i, y \in C_j} d(x,y)\]
		\item \textit{Average-Link}:\\
		\[c(C_i,C_j) = \frac{1}{|C_i||C_j|}\displaystyle\sum_{x \in C_i} \displaystyle\sum_{y \in C_j} d(x,y)\]
		\item \textit{Complete-Link}:\\
		\[c(C_i,C_j) = \max\limits_{x \in C_i, y \in C_j} d(x,y)\]\\
\end{itemize}

\cite{fasulo99, jain99, mitch97}
   
\subsubsection{Zusammenfassung}
Die Datenmatrix bzw. der Datenvektor spielen bei dieser Methodik eine geringe Rolle, da eigentlich beim Clustering speziell beim hierarchischen meistens auf die Distanzen Rücksicht genommen wird und diese als Eingabe eingesetzt werden. Meist werden diese Distanzen durch eine Matrix repräsentiert.  Die Dimensionen werden als \(n \times n\) dargestellt, welche bei großen Datenmengen oder großen \(n\) zu Speicherproblemen führen können, da die Datenmenge sehr schnell ansteigen kann. \\
Abhilfe kann geschaffen werden, indem ein Schwellenwert festgelegt wird. Damit werden unbedeutende Wertepaare vernachlässigt. Weiters können auch die Anzahl der verwendeten Elemente begrenzt werden und dadurch die repräsentative Menge der Elemente reduziert werden. Auch können die Verlinkungen zu Nachbarn begrenzt werden um die Anzahl der nächsten Nachbarn zu begrenzen. \\


\subsection{Self-organizing Maps}
\subsubsection{Beschreibung:}
Bei diesen Clusteringverfahren wird ein mehrdimensionaler Datensatz mit einer bestimmten Dimensionalität auf ein Gitter mit wenig Dimensionalität meist ein - oder zweidimensional projiziert. Die Anzahl der Cluster ist festgelegt und kann durch die Spalten und Zeilen im Gitter festgelegt werden. Die Referenzvektoren werden beim SOM Clustering aus den Knoten im Gitter gebildet und durch eine iterative Annäherung anhand des vorgegeben Algorithmus zu den Eingabevektoren geleitet.   \cite{tam99}
\subsubsection{Algorithmus:}

\begin{enumerate}
	\item Wähle ein Gitter mit \(k = k_v \times k_h\) Knoten \(v,v \in \{1,\dotsc,k\}\).\\
	
	\item Initialisiere \(k\) \(d\)-dimensionale Vektoren \(f_0(v)\), durch zufällige Wahl von Objekten \(x \in S\) oder vollständig zufällig\\
	
	\item Iteration \(i\):
	\begin{enumerate}
		\item Für jedes Objekt \(x \in S\) bestimme den Knoten \(v_x\), für den \(f_i(v_x)\) am nächsten zu \(x\) liegt.\\
		
		\item Aktualisiere alle Referenzvektoren wie folgt:\\
		
		\[f_{i+1} = f_i(v) + \eta(d(v,_x),i) \cdot (x - f_i(v)) \]\\
		
		\(\eta(d,i)\): Lernrate; Die Lernrate nimmt mit der Distanz zwischen den Knoten und der Iteration ab.\\
		\item Wiederhole (a) und (b) bis keine Veränderung mehr eintritt.\\
	\end{enumerate} 
\end{enumerate}
\cite{tam99}
\subsubsection{Zusammenfassung}
Bei dem SOM-Algorithmus ist die Reduktion der Dimensionalität an erster Stelle und kann somit sehr effizient zum Berechnen von großen Datensätzen herangezogen werden, da die Dimensionalität bzw. die Anzahl der Merkmale gering ist. Auch eignet sich der Algorithmus um Daten aufzuteilen, da die Clusteranzahl vorbestimmt ist und so ein vereinfachter Algorithmus angewendet werden kann. 

\subsection{Graph-based Clustering}
Beim \textit{Graph-based Clustering} wird von einem Graphen ausgegangen, welcher die Distanzmatrix repräsentiert. Die Knoten im Graphen werden Objekten aus der Datenmenge zugeordnet. Die verbindenden Linien oder auch Kanten entsprechen der Distanz zwischen den einzelnen Objekten. Diese können \textit{gerichtet} oder \textit{ungerichtet} sein.\cite{fasulo99,jain99} \\
Das Verfahren versucht den Graphen zu zerteilen und ist auch in der Heuristik als Graphpartionsproblem bekannt. Meistens wird eine rekursive Bipartitionierung angenommen, da diese sehr effizient zu berechnen ist. Dabei sind die Graphen eine repräsentative Darstellung von Ähnlichkeitsbeziehungen der einzelnen Objekte.\cite{jain99}\\
Eine weit verbreitete Methode bei Clustering mit Hilfe von Graphen wird \textit{Clique-based Clustering} genannt. Dabei wird durch die Cliquengraphen die Beziehungen zwischen Objekten und deren Ähnlichkeit gezeigt. Im Idealfall sind die Objekte in einen Cluster sehr ähnlich zueinander und die Objekte die zu anderen Clustern gehören sind unähnlich zueinander. Dabei sind die Knoten im Graph die Objekte und die Cliquen sind die Cluster. Die Kanten symbolisieren dass die Elemente ähnlich zueinander sind.\cite{fasulo99,jain99}\\
Doch in der Praxis können Ähnlichkeitsbeziehungen nur bedingt durch Cliquengraphen dargestellt werden , da Kanten fehlen oder Kanten mehrfach vorhanden sein können. Es gibt ein Modell welches sich \textit{corrupted clique graph Model} nennt, bei dem die Kanten die Wahrscheinlichkeiten darstellen und gewichtet sind. Es wird versucht vom \textit{corrupted} zum originalen Graphen zu gelangen, welcher die richtigen Cluster repräsentiert.\\ \cite{fasulo99}

\subsection{Spectral Clustering} 
\subsubsection{Beschreibung:}
Das \textit{Spectral Clustering} ist ein Partitionierungsalgorithmus, welcher die Eigenvektoren der einzelnen Cluster verwendet und dann damit eine Beziehung zu anderen Clustern erstellt. Bei diesen Verfahren wird die Ähnlichkeitsmatrix herangezogen und die Anzahl der Cluster kann vom Benutzer festgelegt werden.
\subsubsection{Algorithmus}
Gegeben ist die Datenmenge \(S = \{x_1,\dotsc,x_n\} \text{ im }\mathbb{R}^d\); \(k:\) : Clusteranzahl\\

\begin{enumerate}
	\item Berechne die Ähnlichkeitsmatrix \(A_{n\times n}\)\\
	
	\[A_{ij} =  
	\begin{cases}
    e^{\frac{-|x_i-x_j|^2}{2\sigma^2} }      & \quad \text{falls } i \neq j\\
    0 & \quad \text{ sonst}
  \end{cases}\]\\
	
	\[\sigma^2\text{ : Skalierungsfaktor}\]
	\item Berechne die Diagonalmatrix \(D_{n\times n}\)\\
	\[D_{ij} = 
		\begin{cases}
    \displaystyle\sum_{l=1}^n  A_{ij}    & \quad \text{falls } i = j\\
    0 & \quad \text{ sonst}
	
  \end{cases}\]\\
	
		In der Diagonale \(D\) stehen die Zeilensummen von \(A\)\\
		Berechne \(L = D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\)\\
				\[D^{-\frac{1}{2}} =
		\begin{cases}
    \frac{1}{\sqrt{D_{ij}}}    & \quad \text{falls } i = j\\
    0 & \quad \text{ sonst}
	
  \end{cases}\]\\
	
	\item Finde \(v_1,\dotsc,v_k\), die \(k\) größten Eigenvektoren von \(L\), so dass alle \(v_i\) paarweise orthogonal sind. Erstelle daraus eine Matrix\\
	
	\[X_{n\times k} = [v_1,\dotsc,v_k] \in \mathbb{R}^{n\times k}\]\\
	
	
	\item Konsruiere Matrix \(Y_{n\times k}\) durch normalisierung von \(X\)\\
	
	\[Y_{ij} = \frac{X_{ij}}{\sqrt{ \sum_{j} X_{ij}^2}}\]\\
	
	\item Jede Zeile \(Y_i\) von Y ist ein Punkt \(\mathbb{R}^k\) im Clustere diese Punkte mit einem beliebigen Clusteralgorithmus\\
	
	\item Weise jedem Originalpunkt \(x_i\) den Cluster \(j\) genau dann zu, wenn die Zeile \(Yi\) im Cluster \(j\)liegt.\\
	
\end{enumerate}
\cite{fay}

Beim spektralen Clustering ist die Form des Cluster nicht so bedeutsam, da die Form von den eingegeben Daten abhängt und es eine recht einfache Implementierung mit verschieden Sprachen gibt. Die Clusteranzahl muss vorher gewählt werden und dies kann sich unter Umständen schwierig gestalten.\\   \cite{fasulo99,mitch97,alp95}
\section{Anwendungsgebiete}
Das Clustering hat viele Anwendungsgebiete, da in der Informatik und Statistik große Dateien und Datensätze vorkommen. Clustering und Klassifikation sind vor allen in der Heuristik sehr bedeutsam. Auch in der Medizin spielt Clustering eine wichtige Rolle, da in den Anwendungen in der Medizin Daten von Patienten in Klassen eingeteilt werden müssen wie z.B. AML/ALL Klassifikation der Krebsmerkmale. Auch können in der Biologie große Datenmengen anfallen. Diese müssen aufgeteilt werden und in Gruppen eingeteilt werden wie z.B. die Zuordnung von Primer an der DNA/RNA.\cite{fay} \\ 
Wie schon oben beschrieben spielt das Clustering als ein Verfahren für die Klassifikation in vielen Anwendungsgebieten eine bedeutende Rolle und wird auch gerne als Hilfsmittel für diverse Berechnungen herangezogen. Das Clustering wird häufig in Verbindung mit der Klassifikation eingesetzt und damit wird noch eine breitere Anwendung ermöglicht. \cite{fay}

\section{Visualisierung}
Um die Resultate und Ergebnisse betrachten und analysieren zu können, müssen die Daten und deren dazugehörigen Ergebnisse darstellt bzw. visualisiert werden. Dazu werden  Diagramme, Graphen oder repräsentative graphische Darstellungen verwendet, welche nur einen kleinen Teil der verwendeten Daten darstellen. Dadurch kann man sehr repräsentative Ergebnisse darstellen. \\
Es wird für eine Reihe von Ergebnissen z.B. eine Messreihe mit einer zwei- oder dreidimensionale Darstellungsmethode gewählt, welche Heatmaps oder Fitnesslandschaften darstellen. Dabei können die Beziehungen und Zusammenhänge sehr gut dargestellt werden. Bei der Auswertung der Daten können hierarchische Daten bzw. Ergebnisse entstehen. Dann sollte eine andere Darstellungsform gewählt werden, welche die hierarchische Ordnung von den Daten berücksichtigt. Solche Art von Diagrammen werden Dendrogramm genannt. Dabei wird die hierarchische Ordnung als Baumstruktur verwendet. Damit kann der Verlauf von einzelnen Clusteringschritten  nachverfolgt werden. 

\section{Zusammenfassung und Ausblick}
Das Clustering, welches hier dargestellt wurde, ist eine Methode der unüberwachten Klassifikation. Die verschieden Algorithmen helfen große und komplexe Datenmengen besser zu analysieren und erleichtern die nachträgliche Verarbeitung. Diese müssen interpretiert werden wobei die verschieden Visualisierungsmöglichkeiten hilfreich sind. Wie gut das Clustering interpretiert werden kann hängt sehr stark von den gewählten Parametern bzw. vom gewählten Algorithmus ab. \\
Die verwendeten Daten entscheiden über die Art des gewählten Algorithmus, da bei manchen Datensätzen Algorithmen keine Ergebnisse liefern, da es für jede Methode Voraussetzungen gibt. So ist es wichtig zuerst abzuklären, welche Voraussetzungen gegeben sind. Dann sollte der richtige Algorithmus ausgewählt werden und nicht umgekehrt.\\
Diese Algorithmen würden bessere Ergebnisse liefern, wenn die Daten besser angepasst wären, da es bei der Effizienz und Performance meist nur auf die Beschaffenheit der Daten ankommt. Die richtige Wahl der Parameter ist die größte Herausforderung beim Clustering und auch bei den heuristischen Algorithmen und Verfahren. Da häufig kein Wissen über das Verhalten von den Daten \textit{a priori}  bekannt ist, ist eine Vorhersage nur schwer möglich. \\
In Zukunft wird die Entwicklung in Richtung selbst adaptiver Clusteringalgorithmen gehen, welche sich anhand der Daten anpassen und nicht mehr auf Annahmen basieren, welche meist nur sehr schlecht performante Ergebnisse liefern. Auch muss hier die Effizienz per Datensatz gesteigert werden um zeitlich bessere Ergebnisse zu erhalten.